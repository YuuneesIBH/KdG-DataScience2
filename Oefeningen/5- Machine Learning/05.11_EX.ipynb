{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Exercises: Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:20:34.839450Z",
     "start_time": "2025-04-23T07:20:34.130925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Wine dataset\n",
    "This exercise is base on the wine dataset included in the sklearn.datasets module.\n",
    "In the Jupyter tab explore the 'wine' object. It stores all kinds of data about the wine dataset. You will probably need data, feature_names, target and target_names. In the following cell the wine dataset is loaded."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### Data preperation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:51:15.758226Z",
     "start_time": "2025-04-23T07:51:15.718545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The dataset is loaded for you.\n",
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target)\n",
    "X.head()\n",
    "y.head()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "a. It is often a good idea to normalize the dataset. Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. One of the most used normalization techniques is transforming your input data to a set with a mean of 0 and a standard deviation of 1. (calculating Z-scores).\\\n",
    "In sklearn this can be done with the StandardScaler.\\\n",
    "Create a new dataframe with the normalized data.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:51:19.836034Z",
     "start_time": "2025-04-23T07:51:19.812369Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": [
    "### Model selection and hyperparameter selection\n",
    " a.  Fit a K-means with 3 clusters on this set. Use random_state=42 to ensure the same results every time.\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:51:23.635662Z",
     "start_time": "2025-04-23T07:51:23.628974Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b. Derive the model and predict the clusters."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:51:27.113280Z",
     "start_time": "2025-04-23T07:51:27.054645Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "c. Instead of performing the Scaler seperatly, use the pipeline module from sklearn. A Pipeline is a sequence of data tranformers (in this case the StandardScaler() with an optional final predictor (in this case KMeans). You already used pipeline to transform a polynominal regression to a linear regression.  \\\n",
    "Create a pipeline that first executes the StandardScaler() and uses the output to apply Kmeans and the KMeans."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:57:10.855488Z",
     "start_time": "2025-04-23T07:57:10.829700Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "d. Create a dataframe with the resulting 'clusters' and the original 'wine classes'.\\\n",
    "e. Can you see a pattern when comparing the clusters with the original wine classes?"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T07:57:17.188338Z",
     "start_time": "2025-04-23T07:57:17.175556Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "e. Visualize the clusters by plotting the first two features. Color the points by cluster.\\"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:58:34.937882Z",
     "start_time": "2025-04-23T07:58:34.450897Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "f. Now do the same for every successive feature combination. ex. feature1 vs feature2, feature2 vs feature3, feature3 vs feature4, etc.\\\n",
    "Do this by using a 'for' loop and make sure it is reusable for other datasets.\\\n",
    "Tips:\n",
    "- Create a figure with subplots that has n-1 axes (n being the number of features)\n",
    "- loop n-1 times\n",
    "    - Create a scatter-plot on the i-th axe within the loop, and plot feature i and i+1\n",
    "g. Wich combination of features seperates the features in the best way?"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T08:01:21.348733Z",
     "start_time": "2025-04-23T08:01:17.170122Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "# Customer segmentation",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": "This exercise is based on https://www.kaggle.com/code/pablomgomez21/k-means-clustering-practice/notebook but the questions are adjusted to the workflow we use.\n",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data preperation\n",
    "a. Study ``Cust_segmentation.csv`` and import the data with pandas.\\\n",
    "b. Remove 'Adress' as it is Categorical data. Customer_Id should also be removed because it is not useful for clustering.\\\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:42:57.234252Z",
     "start_time": "2025-04-23T08:42:57.196875Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model and hyperparameter selection"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "c. Fit a K-means with 3 clusters and n_init=12 on this set. Use random_state=42 to ensure the same results every time. Make sure the data is normalized before fitting.\\\n",
    "Mind that, even if you do this correctly, you will get an error because of NaN values. Investigate the problem and take a proper action.\\\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:42:58.928534Z",
     "start_time": "2025-04-23T08:42:58.787923Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "d. Display the centers of the clusters. Tip: Use ``named_steps['kmeans']`` to access the Kmeans-model object within the pipeline\\\n",
    "e. The problem here is that you are showing the means of the normalized dataset. Perform an ``inverse_transform()`` on the ``cluster_centers_``. ``inverse_transform()`` is availaible on the ``standardscaler`` step in  the pipeline.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:02:00.729127Z",
     "start_time": "2025-04-23T09:02:00.715694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#d\n",
    "\n",
    "\n",
    "#e\n"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "f. Finally visualize the clusters by plotting 'age' and 'income'. Color the points by cluster.\\"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:48:15.752023Z",
     "start_time": "2025-04-23T08:48:15.372010Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 67
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
